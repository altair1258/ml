{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insurance Claim Prediction - Complete Analysis\n",
    "\n",
    "## Project Overview\n",
    "**Objective**: Build a machine learning model to predict if a building will have an insurance claim during the insurance period\n",
    "\n",
    "**Dataset**:\n",
    "- Training: 5012 observations with 12 descriptive attributes + target variable (Claim)\n",
    "- Test: 2147 observations for final model validation\n",
    "\n",
    "## Project Tasks (Per PDF Requirements):\n",
    "1. **Analyze and visualize data** ✓\n",
    "2. **Clean data if necessary** ✓\n",
    "3. **Select most discriminant features if necessary** ✓ (Using RFE)\n",
    "4. **Encode data and generate prediction models using supervised learning** ✓\n",
    "5. **Evaluate model performance and interpret results** ✓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_selection import RFE, mutual_info_classif\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score, \n",
    "    roc_auc_score, roc_curve, auc, accuracy_score, \n",
    "    precision_score, recall_score\n",
    ")\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"✓ All libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: ANALYZE AND VISUALIZE DATA\n",
    "### 1.1 Load Data and Basic Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(\"train_Insurance.csv\")\n",
    "test_raw = pd.read_csv(\"test_Insurance.csv\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training set shape: {train_raw.shape} (Expected: 5012 rows)\")\n",
    "print(f\"Test set shape: {test_raw.shape} (Expected: 2147 rows)\")\n",
    "print(f\"\\nFeatures ({len(train_raw.columns)}): {list(train_raw.columns)}\")\n",
    "\n",
    "print(f\"\\n{'-'*80}\")\n",
    "print(\"First 10 rows of training data:\")\n",
    "print(train_raw.head(10))\n",
    "\n",
    "print(f\"\\n{'-'*80}\")\n",
    "print(\"Data types:\")\n",
    "print(train_raw.dtypes)\n",
    "\n",
    "print(f\"\\n{'-'*80}\")\n",
    "print(\"Statistical summary:\")\n",
    "print(train_raw.describe())\n",
    "\n",
    "print(f\"\\n{'-'*80}\")\n",
    "print(\"Dataset information:\")\n",
    "train_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "missing_train = train_raw.isna().sum()\n",
    "missing_train_pct = (missing_train / len(train_raw)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_train.index,\n",
    "    'Missing_Count': missing_train.values,\n",
    "    'Missing_Percentage': missing_train_pct.values\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(\"\\nMissing values detected:\")\n",
    "    print(missing_df.to_string(index=False))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].barh(missing_df['Column'], missing_df['Missing_Count'], color='coral')\n",
    "    axes[0].set_xlabel('Missing Count', fontweight='bold')\n",
    "    axes[0].set_title('Missing Values Count by Column', fontweight='bold', fontsize=12)\n",
    "    axes[0].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    axes[1].barh(missing_df['Column'], missing_df['Missing_Percentage'], color='steelblue')\n",
    "    axes[1].set_xlabel('Missing Percentage (%)', fontweight='bold')\n",
    "    axes[1].set_title('Missing Values Percentage by Column', fontweight='bold', fontsize=12)\n",
    "    axes[1].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n✓ No missing values found in training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Target Variable (Claim) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TARGET VARIABLE ANALYSIS: Claim\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "claim_counts = train_raw['Claim'].value_counts()\n",
    "claim_pct = train_raw['Claim'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"\\nClaim Distribution:\")\n",
    "for label in claim_counts.index:\n",
    "    print(f\"  {label}: {claim_counts[label]} samples ({claim_pct[label]:.2f}%)\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "train_raw['Claim'].value_counts().plot.pie(\n",
    "    autopct='%1.1f%%',\n",
    "    ax=axes[0],\n",
    "    ylabel='',\n",
    "    colors=['#66b3ff', '#ff9999'],\n",
    "    startangle=90,\n",
    "    explode=(0.05, 0.05)\n",
    ")\n",
    "axes[0].set_title('Target Distribution (Pie Chart)', fontweight='bold', fontsize=12)\n",
    "\n",
    "claim_counts.plot(kind='bar', ax=axes[1], color=['#66b3ff', '#ff9999'], width=0.6)\n",
    "axes[1].set_title('Target Distribution (Bar Chart)', fontweight='bold', fontsize=12)\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_xlabel('Claim Status')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=0)\n",
    "for i, v in enumerate(claim_counts.values):\n",
    "    axes[1].text(i, v + 50, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "claim_pct.plot(kind='barh', ax=axes[2], color=['#66b3ff', '#ff9999'])\n",
    "axes[2].set_title('Target Distribution (Percentage)', fontweight='bold', fontsize=12)\n",
    "axes[2].set_xlabel('Percentage (%)')\n",
    "axes[2].set_ylabel('Claim Status')\n",
    "for i, v in enumerate(claim_pct.values):\n",
    "    axes[2].text(v + 1, i, f\"{v:.2f}%\", ha='left', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Categorical Features Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [col for col in train_raw.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "            if col not in [\"Claim\", \"Customer Id\"]]\n",
    "\n",
    "print(f\"Categorical features ({len(cat_cols)}): {cat_cols}\")\n",
    "\n",
    "n_cols = 3\n",
    "n_rows = math.ceil(len(cat_cols) / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5 * n_rows))\n",
    "axes = axes.flatten() if isinstance(axes, np.ndarray) else [axes]\n",
    "\n",
    "for i, col in enumerate(cat_cols):\n",
    "    value_counts = train_raw[col].value_counts()\n",
    "    sns.countplot(data=train_raw, x=col, ax=axes[i], palette='Set2', order=value_counts.index)\n",
    "    axes[i].set_title(f\"Distribution of {col}\", fontweight='bold')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for container in axes[i].containers:\n",
    "        axes[i].bar_label(container)\n",
    "\n",
    "for j in range(len(cat_cols), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Categorical Features vs Target (Claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CATEGORICAL FEATURES vs TARGET ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5 * n_rows))\n",
    "axes = axes.flatten() if isinstance(axes, np.ndarray) else [axes]\n",
    "\n",
    "for i, col in enumerate(cat_cols):\n",
    "    ct = pd.crosstab(train_raw[col], train_raw['Claim'], normalize='index')\n",
    "    ct.plot(kind='bar', ax=axes[i], stacked=False, color=['#66b3ff', '#ff9999'])\n",
    "    axes[i].set_title(f\"{col} vs Claim\", fontweight='bold')\n",
    "    axes[i].set_ylabel('Proportion')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].legend(title='Claim', loc='best')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "for j in range(len(cat_cols), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Numerical Features Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = train_raw.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "num_cols = [c for c in num_cols if c not in [\"Claim\", \"Customer Id\"]]\n",
    "\n",
    "print(f\"Numerical features ({len(num_cols)}): {num_cols}\")\n",
    "\n",
    "n_cols_plot = 3\n",
    "n_rows_plot = math.ceil(len(num_cols) / n_cols_plot)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows_plot, n_cols_plot, figsize=(18, 5 * n_rows_plot))\n",
    "axes = axes.flatten() if isinstance(axes, np.ndarray) else [axes]\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    sns.histplot(train_raw[col].dropna(), kde=True, ax=axes[i], color='steelblue', bins=30)\n",
    "    axes[i].set_title(f\"Distribution of {col}\", fontweight='bold')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "for j in range(len(num_cols), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Numerical Features: Box Plots (Outlier Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(n_rows_plot, n_cols_plot, figsize=(18, 5 * n_rows_plot))\n",
    "axes = axes.flatten() if isinstance(axes, np.ndarray) else [axes]\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    sns.boxplot(x=train_raw[col].dropna(), ax=axes[i], color='orange')\n",
    "    axes[i].set_title(f\"Box Plot: {col}\", fontweight='bold')\n",
    "    axes[i].set_xlabel(col)\n",
    "\n",
    "for j in range(len(num_cols), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOutlier Analysis (IQR method):\")\n",
    "print(\"-\" * 60)\n",
    "for col in num_cols:\n",
    "    Q1 = train_raw[col].quantile(0.25)\n",
    "    Q3 = train_raw[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    outliers = ((train_raw[col] < lower) | (train_raw[col] > upper)).sum()\n",
    "    outlier_pct = (outliers / len(train_raw)) * 100\n",
    "    print(f\"{col}:\")\n",
    "    print(f\"  Outliers: {outliers} ({outlier_pct:.2f}%)\")\n",
    "    print(f\"  Expected range: [{lower:.2f}, {upper:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Numerical Features vs Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(n_rows_plot, n_cols_plot, figsize=(18, 5 * n_rows_plot))\n",
    "axes = axes.flatten() if isinstance(axes, np.ndarray) else [axes]\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    for claim_val in train_raw['Claim'].unique():\n",
    "        data = train_raw[train_raw['Claim'] == claim_val][col].dropna()\n",
    "        axes[i].hist(data, alpha=0.6, label=claim_val, bins=30)\n",
    "    axes[i].set_title(f\"{col} by Claim Status\", fontweight='bold')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].legend(title='Claim')\n",
    "\n",
    "for j in range(len(num_cols), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2: CLEAN DATA IF NECESSARY\n",
    "### 2.1 Remove Customer ID (Not Predictive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DATA CLEANING PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nStep 1: Remove Customer ID\\n\" + \"-\"*60)\n",
    "\n",
    "if 'Customer Id' in train_raw.columns:\n",
    "    train_raw = train_raw.drop('Customer Id', axis=1)\n",
    "    print(\"✓ Customer Id removed from training data\")\n",
    "\n",
    "if 'Customer Id' in test_raw.columns:\n",
    "    test_raw = test_raw.drop('Customer Id', axis=1)\n",
    "    print(\"✓ Customer Id removed from test data\")\n",
    "\n",
    "print(f\"\\nRemaining columns ({len(train_raw.columns)}): {list(train_raw.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Clean NumberOfWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStep 2: Clean NumberOfWindows\\n\" + \"-\"*60)\n",
    "\n",
    "print(f\"Unique values before cleaning: {sorted(train_raw['NumberOfWindows'].unique())}\")\n",
    "\n",
    "train_raw['NumberOfWindows'] = train_raw['NumberOfWindows'].replace({'without': 0, '>=10': 10})\n",
    "train_raw['NumberOfWindows'] = train_raw['NumberOfWindows'].astype(int)\n",
    "\n",
    "test_raw['NumberOfWindows'] = test_raw['NumberOfWindows'].replace({'without': 0, '>=10': 10})\n",
    "test_raw['NumberOfWindows'] = test_raw['NumberOfWindows'].astype(int)\n",
    "\n",
    "print(f\"Unique values after cleaning: {sorted(train_raw['NumberOfWindows'].unique())}\")\n",
    "print(\"✓ NumberOfWindows cleaned successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Remove Duplicates and Conflicting Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStep 3: Remove Duplicates and Conflicts\\n\" + \"-\"*60)\n",
    "\n",
    "n_exact = train_raw.duplicated().sum()\n",
    "features = [c for c in train_raw.columns if c != \"Claim\"]\n",
    "n_same_features = train_raw.duplicated(subset=features).sum()\n",
    "\n",
    "print(f\"Exact duplicates: {n_exact}\")\n",
    "print(f\"Records with same features (may have different target): {n_same_features}\")\n",
    "\n",
    "if n_same_features > 0:\n",
    "    dups = train_raw[train_raw.duplicated(subset=features, keep=False)]\n",
    "    conflicts = dups.groupby(features)['Claim'].nunique()\n",
    "    n_conflicts = (conflicts > 1).sum()\n",
    "    print(f\"Conflicting records (same features, different targets): {n_conflicts}\")\n",
    "    \n",
    "    if n_conflicts > 0:\n",
    "        conflicting_groups = conflicts[conflicts > 1].reset_index()\n",
    "        before = len(train_raw)\n",
    "        train_raw = train_raw.merge(conflicting_groups[features], on=features, how='left', indicator=True)\n",
    "        train_raw = train_raw[train_raw['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "        after = len(train_raw)\n",
    "        print(f\"✓ Removed {before - after} conflicting records\")\n",
    "\n",
    "before_dup = len(train_raw)\n",
    "train_raw = train_raw.drop_duplicates()\n",
    "train_raw = train_raw.reset_index(drop=True)\n",
    "after_dup = len(train_raw)\n",
    "\n",
    "if before_dup > after_dup:\n",
    "    print(f\"✓ Removed {before_dup - after_dup} duplicate records\")\n",
    "else:\n",
    "    print(\"✓ No duplicates found\")\n",
    "\n",
    "print(f\"\\nTrain shape after cleaning: {train_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStep 4: Handle Missing Values\\n\" + \"-\"*60)\n",
    "\n",
    "print(\"Missing values before imputation:\")\n",
    "missing_before = train_raw.isna().sum()[train_raw.isna().sum() > 0]\n",
    "if len(missing_before) > 0:\n",
    "    print(missing_before)\n",
    "else:\n",
    "    print(\"None (except Geo_Code, handled separately)\")\n",
    "\n",
    "mf_imputer = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\n",
    "if train_raw[\"Garden\"].isna().sum() > 0:\n",
    "    train_raw[[\"Garden\"]] = mf_imputer.fit_transform(train_raw[[\"Garden\"]])\n",
    "    print(\"\\n✓ Garden: Imputed with most frequent value\")\n",
    "\n",
    "median_imputer = SimpleImputer(strategy=\"median\")\n",
    "if train_raw[\"Building Dimension\"].isna().sum() > 0:\n",
    "    train_raw[[\"Building Dimension\"]] = median_imputer.fit_transform(train_raw[[\"Building Dimension\"]])\n",
    "    print(\"✓ Building Dimension: Imputed with median\")\n",
    "\n",
    "print(f\"\\nMissing values after imputation:\")\n",
    "remaining = train_raw.isna().sum()[train_raw.isna().sum() > 0]\n",
    "if len(remaining) > 0:\n",
    "    print(remaining)\n",
    "else:\n",
    "    print(\"None (except Geo_Code)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Fill Missing Geo_Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStep 5: Fill Missing Geo_Code\\n\" + \"-\"*60)\n",
    "\n",
    "print(f\"Missing Geo_Code before filling: {train_raw['Geo_Code'].isna().sum()}\")\n",
    "\n",
    "mode_geo_train = (\n",
    "    train_raw[train_raw[\"Geo_Code\"].notna()]\n",
    "    .groupby([\"Settlement\", \"Residential\"])[\"Geo_Code\"]\n",
    "    .agg(lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0])\n",
    "    .reset_index()\n",
    "    .rename(columns={\"Geo_Code\": \"Geo_Code_mode\"})\n",
    ")\n",
    "\n",
    "print(\"\\nGeo_Code modes by Settlement + Residential:\")\n",
    "print(mode_geo_train)\n",
    "\n",
    "train_raw = train_raw.merge(mode_geo_train, on=[\"Settlement\", \"Residential\"], how=\"left\")\n",
    "train_raw[\"Geo_Code\"] = train_raw[\"Geo_Code\"].fillna(train_raw[\"Geo_Code_mode\"])\n",
    "train_raw = train_raw.drop(columns=[\"Geo_Code_mode\"])\n",
    "\n",
    "print(f\"\\n✓ Missing Geo_Code after filling: {train_raw['Geo_Code'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Clean Geo_Code (Remove Alphanumeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStep 6: Clean Geo_Code (Remove Alphanumeric)\\n\" + \"-\"*60)\n",
    "\n",
    "mask_numeric = train_raw[\"Geo_Code\"].astype(str).str.isnumeric()\n",
    "print(f\"Numeric Geo_Code: {mask_numeric.sum()}\")\n",
    "print(f\"Alphanumeric Geo_Code: {(~mask_numeric).sum()}\")\n",
    "\n",
    "if (~mask_numeric).sum() > 0:\n",
    "    train_raw = train_raw[mask_numeric].copy()\n",
    "    train_raw[\"Geo_Code\"] = train_raw[\"Geo_Code\"].astype(int)\n",
    "    train_raw = train_raw.reset_index(drop=True)\n",
    "    print(f\"\\n✓ Removed alphanumeric Geo_Code entries\")\n",
    "else:\n",
    "    train_raw[\"Geo_Code\"] = train_raw[\"Geo_Code\"].astype(int)\n",
    "\n",
    "print(f\"Train shape after Geo_Code cleaning: {train_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Handle Outliers in Building Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStep 7: Handle Outliers in Building Dimension\\n\" + \"-\"*60)\n",
    "\n",
    "Q1 = train_raw['Building Dimension'].quantile(0.25)\n",
    "Q3 = train_raw['Building Dimension'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}\")\n",
    "print(f\"Valid range: [{lower:.2f}, {upper:.2f}]\")\n",
    "\n",
    "outliers_before = ((train_raw['Building Dimension'] < lower) | (train_raw['Building Dimension'] > upper)).sum()\n",
    "print(f\"Outliers detected: {outliers_before}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.boxplot(x=train_raw['Building Dimension'], ax=axes[0], color='orange')\n",
    "axes[0].set_title('Building Dimension - Before Outlier Treatment', fontweight='bold')\n",
    "\n",
    "train_raw['Building Dimension'] = train_raw['Building Dimension'].clip(lower, upper)\n",
    "\n",
    "sns.boxplot(x=train_raw['Building Dimension'], ax=axes[1], color='skyblue')\n",
    "axes[1].set_title('Building Dimension - After Outlier Treatment (Clipping)', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Outliers clipped to range [{lower:.2f}, {upper:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Scale Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStep 8: Scale Numerical Features\\n\" + \"-\"*60)\n",
    "\n",
    "cols_to_scale = ['Building Dimension', 'NumberOfWindows']\n",
    "scaler = RobustScaler()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for i, col in enumerate(cols_to_scale):\n",
    "    sns.histplot(train_raw[col], kde=True, ax=axes[i, 0], color='coral', bins=30)\n",
    "    axes[i, 0].set_title(f\"{col} - Before Scaling\", fontweight='bold')\n",
    "\n",
    "train_raw[cols_to_scale] = scaler.fit_transform(train_raw[cols_to_scale])\n",
    "\n",
    "for i, col in enumerate(cols_to_scale):\n",
    "    sns.histplot(train_raw[col], kde=True, ax=axes[i, 1], color='steelblue', bins=30)\n",
    "    axes[i, 1].set_title(f\"{col} - After RobustScaler\", fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ RobustScaler fitted and applied to training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStep 9: Encode Categorical Variables\\n\" + \"-\"*60)\n",
    "\n",
    "train_transformed = train_raw.copy()\n",
    "\n",
    "train_transformed[\"Building_Painted\"] = train_transformed[\"Building_Painted\"].map({'N': 1, 'V': 0}).astype('int32')\n",
    "train_transformed[\"Building_Fenced\"] = train_transformed[\"Building_Fenced\"].map({'N': 1, 'V': 0}).astype('int32')\n",
    "train_transformed[\"Garden\"] = train_transformed[\"Garden\"].map({'V': 1, 'O': 0}).astype('int32')\n",
    "\n",
    "print(\"✓ Binary encoding: Building_Painted, Building_Fenced, Garden\")\n",
    "\n",
    "train_transformed = pd.get_dummies(train_transformed, columns=[\"Settlement\", \"Building_Type\"], drop_first=True, dtype='int32')\n",
    "\n",
    "print(\"✓ One-hot encoding: Settlement, Building_Type\")\n",
    "\n",
    "le_claim = LabelEncoder()\n",
    "train_transformed[\"Claim\"] = le_claim.fit_transform(train_transformed[\"Claim\"])\n",
    "\n",
    "print(f\"✓ Target encoding: Claim (non=0, oui=1)\")\n",
    "\n",
    "cols = [c for c in train_transformed.columns if c != \"Claim\"] + [\"Claim\"]\n",
    "train_transformed = train_transformed[cols]\n",
    "\n",
    "print(f\"\\nShape after encoding: {train_transformed.shape}\")\n",
    "print(f\"Columns: {list(train_transformed.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Initial Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStep 10: Correlation Analysis\\n\" + \"-\"*60)\n",
    "\n",
    "df_corr = train_transformed.corr(numeric_only=True)\n",
    "corr_with_claim = df_corr[[\"Claim\"]].sort_values(by=\"Claim\", ascending=False)\n",
    "\n",
    "print(\"\\nCorrelation with Target (Claim):\")\n",
    "print(corr_with_claim)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, max(8, len(corr_with_claim) * 0.35)))\n",
    "\n",
    "sns.heatmap(corr_with_claim, annot=True, fmt='.3f', vmin=-1, vmax=1, \n",
    "            cmap='coolwarm', center=0, ax=axes[0], cbar_kws={'label': 'Correlation'})\n",
    "axes[0].set_title('Feature Correlation with Target (Claim)', fontsize=14, fontweight='bold')\n",
    "\n",
    "sns.heatmap(df_corr, annot=False, vmin=-1, vmax=1, cmap='coolwarm', \n",
    "            center=0, ax=axes[1], cbar_kws={'label': 'Correlation'})\n",
    "axes[1].set_title('Full Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CORRELATION INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "max_corr = corr_with_claim.drop('Claim').abs().max()[0]\n",
    "print(f\"\\nMaximum absolute correlation with target: {max_corr:.4f}\")\n",
    "\n",
    "if max_corr < 0.3:\n",
    "    print(\"\\n⚠ WARNING: VERY LOW CORRELATION DETECTED!\")\n",
    "    print(\"\\nThis suggests:\")\n",
    "    print(\"  1. Weak linear relationships between features and target\")\n",
    "    print(\"  2. Non-linear relationships may be present (good for tree-based models)\")\n",
    "    print(\"  3. Feature interactions may be more important than individual features\")\n",
    "    print(\"  4. Complex patterns that require ensemble methods\")\n",
    "    print(\"\\nRECOMMENDATION:\")\n",
    "    print(\"  ✓ Prioritize tree-based models: Random Forest, Gradient Boosting, Extra Trees\")\n",
    "    print(\"  ✓ Consider feature interactions and polynomial features\")\n",
    "    print(\"  ✓ Ensemble methods will likely perform best\")\n",
    "    print(\"  ✗ Linear models may struggle with this dataset\")\n",
    "elif max_corr < 0.5:\n",
    "    print(\"\\n✓ Moderate correlation detected\")\n",
    "    print(\"  - Both linear and non-linear models may work well\")\n",
    "else:\n",
    "    print(\"\\n✓ Strong correlation detected\")\n",
    "    print(\"  - Linear models should perform well\")\n",
    "\n",
    "weak_features = corr_with_claim.drop('Claim')[corr_with_claim.drop('Claim')['Claim'].abs() < 0.05]\n",
    "if len(weak_features) > 0:\n",
    "    print(f\"\\nFeatures with very weak correlation (|r| < 0.05): {len(weak_features)}\")\n",
    "    for feat in weak_features.index:\n",
    "        print(f\"  - {feat}: {weak_features.loc[feat, 'Claim']:.4f}\")\n",
    "    \n",
    "    cols_to_drop = ['Building_Painted', 'Geo_Code', 'YearOfObservation']\n",
    "    cols_to_drop = [c for c in cols_to_drop if c in train_transformed.columns]\n",
    "    \n",
    "    if len(cols_to_drop) > 0:\n",
    "        train_transformed = train_transformed.drop(columns=cols_to_drop)\n",
    "        print(f\"\\n✓ Dropped very weakly correlated features: {cols_to_drop}\")\n",
    "    \n",
    "    train_transformed = train_transformed.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nFinal feature count: {train_transformed.shape[1] - 1} (excluding target)\")\n",
    "print(f\"Final shape: {train_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 Apply Same Transformations to Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"APPLYING TRANSFORMATIONS TO TEST DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if test_raw[\"Garden\"].isna().sum() > 0:\n",
    "    test_raw[[\"Garden\"]] = mf_imputer.transform(test_raw[[\"Garden\"]])\n",
    "if test_raw[\"Building Dimension\"].isna().sum() > 0:\n",
    "    test_raw[[\"Building Dimension\"]] = median_imputer.transform(test_raw[[\"Building Dimension\"]])\n",
    "\n",
    "test_raw = test_raw.merge(mode_geo_train, on=[\"Settlement\", \"Residential\"], how=\"left\")\n",
    "test_raw[\"Geo_Code\"] = test_raw[\"Geo_Code\"].fillna(test_raw[\"Geo_Code_mode\"])\n",
    "test_raw = test_raw.drop(columns=[\"Geo_Code_mode\"])\n",
    "\n",
    "mask_numeric_test = test_raw[\"Geo_Code\"].astype(str).str.isnumeric()\n",
    "test_raw = test_raw[mask_numeric_test].copy()\n",
    "test_raw[\"Geo_Code\"] = test_raw[\"Geo_Code\"].astype(int)\n",
    "\n",
    "test_raw[cols_to_scale] = scaler.transform(test_raw[cols_to_scale])\n",
    "\n",
    "test_transformed = test_raw.copy()\n",
    "test_transformed[\"Building_Painted\"] = test_transformed[\"Building_Painted\"].map({'N': 1, 'V': 0}).astype('int32')\n",
    "test_transformed[\"Building_Fenced\"] = test_transformed[\"Building_Fenced\"].map({'N': 1, 'V': 0}).astype('int32')\n",
    "test_transformed[\"Garden\"] = test_transformed[\"Garden\"].map({'V': 1, 'O': 0}).astype('int32')\n",
    "\n",
    "test_transformed = pd.get_dummies(test_transformed, columns=[\"Settlement\", \"Building_Type\"], drop_first=True, dtype='int32')\n",
    "test_transformed[\"Claim\"] = le_claim.transform(test_transformed[\"Claim\"])\n",
    "\n",
    "cols = [c for c in test_transformed.columns if c != \"Claim\"] + [\"Claim\"]\n",
    "test_transformed = test_transformed[cols]\n",
    "\n",
    "if 'cols_to_drop' in locals() and len(cols_to_drop) > 0:\n",
    "    cols_to_drop_test = [c for c in cols_to_drop if c in test_transformed.columns]\n",
    "    if len(cols_to_drop_test) > 0:\n",
    "        test_transformed = test_transformed.drop(columns=cols_to_drop_test)\n",
    "\n",
    "test_transformed = test_transformed.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n✓ Test data transformed\")\n",
    "print(f\"Test shape: {test_transformed.shape}\")\n",
    "print(f\"Columns match train: {list(train_transformed.columns) == list(test_transformed.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3: SELECT MOST DISCRIMINANT FEATURES IF NECESSARY\n",
    "### 3.1 Mutual Information Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FEATURE SELECTION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nMethod 1: Mutual Information\\n\" + \"-\"*60)\n",
    "\n",
    "X_all = train_transformed.drop('Claim', axis=1)\n",
    "y_all = train_transformed['Claim']\n",
    "\n",
    "mi_scores = mutual_info_classif(X_all, y_all, random_state=42)\n",
    "mi_df = pd.DataFrame({\n",
    "    'Feature': X_all.columns,\n",
    "    'MI_Score': mi_scores\n",
    "}).sort_values('MI_Score', ascending=False)\n",
    "\n",
    "print(\"\\nMutual Information Scores (higher = more informative):\")\n",
    "print(mi_df.to_string(index=False))\n",
    "\n",
    "plt.figure(figsize=(10, max(6, len(mi_df) * 0.4)))\n",
    "plt.barh(mi_df['Feature'], mi_df['MI_Score'], color='teal')\n",
    "plt.xlabel('Mutual Information Score', fontweight='bold')\n",
    "plt.title('Feature Importance: Mutual Information with Target', fontweight='bold', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTop 5 most informative features:\")\n",
    "for i, row in mi_df.head(5).iterrows():\n",
    "    print(f\"  {row['Feature']}: {row['MI_Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMethod 2: Recursive Feature Elimination (RFE)\\n\" + \"-\"*60)\n",
    "\n",
    "rf_estimator = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10, n_jobs=-1)\n",
    "n_features_to_select = max(5, len(X_all.columns) // 2)\n",
    "\n",
    "print(f\"Using Random Forest as estimator\")\n",
    "print(f\"Selecting top {n_features_to_select} features out of {len(X_all.columns)}...\\n\")\n",
    "\n",
    "rfe = RFE(estimator=rf_estimator, n_features_to_select=n_features_to_select, step=1)\n",
    "rfe.fit(X_all, y_all)\n",
    "\n",
    "rfe_df = pd.DataFrame({\n",
    "    'Feature': X_all.columns,\n",
    "    'Selected': rfe.support_,\n",
    "    'Ranking': rfe.ranking_\n",
    "}).sort_values('Ranking')\n",
    "\n",
    "print(\"RFE Results:\")\n",
    "print(rfe_df.to_string(index=False))\n",
    "\n",
    "selected_features_rfe = rfe_df[rfe_df['Selected']]['Feature'].tolist()\n",
    "print(f\"\\n✓ RFE selected {len(selected_features_rfe)} features:\")\n",
    "for feat in selected_features_rfe:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colors = ['green' if s else 'red' for s in rfe_df['Selected']]\n",
    "axes[0].barh(rfe_df['Feature'], rfe_df['Ranking'], color=colors, alpha=0.7)\n",
    "axes[0].set_xlabel('Ranking (1 = best)', fontweight='bold')\n",
    "axes[0].set_title('RFE Feature Ranking', fontweight='bold', fontsize=12)\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].axvline(x=1.5, color='blue', linestyle='--', linewidth=2, label='Selection threshold')\n",
    "axes[0].legend()\n",
    "\n",
    "selected_count = rfe_df['Selected'].sum()\n",
    "rejected_count = (~rfe_df['Selected']).sum()\n",
    "axes[1].pie([selected_count, rejected_count], labels=['Selected', 'Rejected'],\n",
    "            autopct='%1.1f%%', colors=['green', 'red'], startangle=90, explode=(0.05, 0))\n",
    "axes[1].set_title(f'Feature Selection Summary\\n({selected_count} selected, {rejected_count} rejected)', \n",
    "                  fontweight='bold', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FEATURE SELECTION DECISION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nOriginal features: {len(X_all.columns)}\")\n",
    "print(f\"RFE recommended: {len(selected_features_rfe)}\")\n",
    "print(f\"\\nNote: We will test models with BOTH:\")\n",
    "print(f\"  1. Full feature set ({len(X_all.columns)} features)\")\n",
    "print(f\"  2. RFE-selected features ({len(selected_features_rfe)} features)\")\n",
    "print(f\"\\nFinal decision will be based on validation performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DATA SPLITTING STRATEGY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_train_full = train_transformed.drop('Claim', axis=1)\n",
    "y_train_full = train_transformed['Claim']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_train_full\n",
    ")\n",
    "\n",
    "X_test = test_transformed.drop('Claim', axis=1)\n",
    "y_test = test_transformed['Claim']\n",
    "\n",
    "print(f\"\\nSplit Configuration:\")\n",
    "print(f\"  Train set:      {X_train.shape[0]:5d} samples ({X_train.shape[1]} features)\")\n",
    "print(f\"  Validation set: {X_val.shape[0]:5d} samples ({X_val.shape[1]} features)\")\n",
    "print(f\"  Test set:       {X_test.shape[0]:5d} samples ({X_test.shape[1]} features)\")\n",
    "\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"  Train:      {dict(y_train.value_counts())}\")\n",
    "print(f\"  Validation: {dict(y_val.value_counts())}\")\n",
    "print(f\"  Test:       {dict(y_test.value_counts())}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "sets = [('Train', y_train), ('Validation', y_val), ('Test', y_test)]\n",
    "\n",
    "for ax, (name, y_data) in zip(axes, sets):\n",
    "    counts = y_data.value_counts()\n",
    "    ax.bar(['No Claim', 'Claim'], counts.values, color=['#66b3ff', '#ff9999'], width=0.6)\n",
    "    ax.set_title(f'{name} Set Distribution', fontweight='bold')\n",
    "    ax.set_ylabel('Count')\n",
    "    for i, v in enumerate(counts.values):\n",
    "        ax.text(i, v + 10, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Class Imbalance Analysis: Resampling Strategy Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CLASS IMBALANCE ANALYSIS & RESAMPLING STRATEGY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "minority_class = y_train.value_counts().min()\n",
    "majority_class = y_train.value_counts().max()\n",
    "imbalance_ratio = majority_class / minority_class\n",
    "minority_percentage = (minority_class / len(y_train)) * 100\n",
    "\n",
    "print(f\"\\nClass Distribution in Training Set:\")\n",
    "print(f\"  Majority class: {majority_class:4d} samples ({100 - minority_percentage:.2f}%)\")\n",
    "print(f\"  Minority class: {minority_class:4d} samples ({minority_percentage:.2f}%)\")\n",
    "print(f\"  Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "print(f\"\\n{'-'*80}\")\n",
    "print(\"SEVERITY ASSESSMENT\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if imbalance_ratio < 1.5:\n",
    "    severity = \"BALANCED\"\n",
    "    color_name = \"green\"\n",
    "    recommendation = \"No special handling needed\"\n",
    "    use_resampling = False\n",
    "elif imbalance_ratio < 3:\n",
    "    severity = \"MILD IMBALANCE\"\n",
    "    color_name = \"yellow\"\n",
    "    recommendation = \"Use class_weight='balanced' parameter\"\n",
    "    use_resampling = False\n",
    "elif imbalance_ratio < 9:\n",
    "    severity = \"MODERATE IMBALANCE\"\n",
    "    color_name = \"orange\"\n",
    "    recommendation = \"Test multiple strategies (class_weight + resampling)\"\n",
    "    use_resampling = True\n",
    "else:\n",
    "    severity = \"SEVERE IMBALANCE\"\n",
    "    color_name = \"red\"\n",
    "    recommendation = \"Resampling strongly recommended\"\n",
    "    use_resampling = True\n",
    "\n",
    "print(f\"Severity Level: {severity}\")\n",
    "print(f\"Recommendation: {recommendation}\")\n",
    "\n",
    "print(f\"\\n{'-'*80}\")\n",
    "print(\"SAMPLE SIZE ANALYSIS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"\\nTotal training samples: {len(X_train)}\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"Samples per feature: {len(X_train) / X_train.shape[1]:.1f}\")\n",
    "print(f\"Minority class samples: {minority_class}\")\n",
    "\n",
    "if minority_class < 50:\n",
    "    size_assessment = \"CRITICAL: Very few minority samples\"\n",
    "    smote_feasible = False\n",
    "elif minority_class < 100:\n",
    "    size_assessment = \"WARNING: Limited minority samples\"\n",
    "    smote_feasible = False\n",
    "elif minority_class < 200:\n",
    "    size_assessment = \"CAUTION: Moderate minority samples\"\n",
    "    smote_feasible = True\n",
    "else:\n",
    "    size_assessment = \"GOOD: Sufficient samples for resampling\"\n",
    "    smote_feasible = True\n",
    "\n",
    "print(f\"\\nSample Size Assessment: {size_assessment}\")\n",
    "print(f\"SMOTE Feasibility: {'✓ Feasible' if smote_feasible else '✗ Not recommended'}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINAL RESAMPLING STRATEGY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not smote_feasible:\n",
    "    use_resampling = False\n",
    "    final_strategy = \"class_weight='balanced' ONLY\"\n",
    "    print(f\"\\nStrategy: {final_strategy}\")\n",
    "    print(\"\\nReason: Insufficient minority samples for reliable synthetic generation\")\n",
    "    print(\"Risk: SMOTE would likely cause overfitting with so few samples\")\n",
    "elif not use_resampling:\n",
    "    final_strategy = \"class_weight='balanced' ONLY\"\n",
    "    print(f\"\\nStrategy: {final_strategy}\")\n",
    "    print(\"\\nReason: Imbalance is mild enough for class weighting alone\")\n",
    "else:\n",
    "    final_strategy = \"TEST MULTIPLE APPROACHES\"\n",
    "    print(f\"\\nStrategy: {final_strategy}\")\n",
    "    print(\"\\nWill compare the following approaches:\")\n",
    "    print(\"  1. Baseline (no handling)\")\n",
    "    print(\"  2. class_weight='balanced'\")\n",
    "    print(\"  3. SMOTE (oversampling minority)\")\n",
    "    print(\"  4. SMOTETomek (hybrid: SMOTE + noise removal)\")\n",
    "    print(\"\\nBest approach will be selected based on validation performance.\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].bar(['Majority\\n(No Claim)', 'Minority\\n(Claim)'], [majority_class, minority_class],\n",
    "            color=['#66b3ff', '#ff9999'], width=0.6)\n",
    "axes[0].set_title('Class Distribution in Training Set', fontweight='bold', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Samples')\n",
    "for i, v in enumerate([majority_class, minority_class]):\n",
    "    axes[0].text(i, v + 10, f\"{v}\\n({[100-minority_percentage, minority_percentage][i]:.1f}%)\", \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "categories = ['Balanced\\n(<1.5:1)', 'Mild\\n(1.5-3:1)', 'Moderate\\n(3-9:1)', 'Severe\\n(>9:1)']\n",
    "thresholds = [1.5, 3, 9, 15]\n",
    "colors_scale = ['green', 'yellow', 'orange', 'red']\n",
    "\n",
    "current_pos = 0\n",
    "for i, threshold in enumerate(thresholds):\n",
    "    if imbalance_ratio <= threshold:\n",
    "        current_pos = i\n",
    "        break\n",
    "else:\n",
    "    current_pos = len(categories) - 1\n",
    "\n",
    "for i, (cat, col) in enumerate(zip(categories, colors_scale)):\n",
    "    alpha = 0.8 if i == current_pos else 0.3\n",
    "    edge_width = 3 if i == current_pos else 0\n",
    "    axes[1].bar(i, thresholds[i], color=col, alpha=alpha, edgecolor='black', linewidth=edge_width)\n",
    "\n",
    "axes[1].axhline(y=imbalance_ratio, color='blue', linestyle='--', linewidth=2,\n",
    "                label=f'Your Data: {imbalance_ratio:.2f}:1')\n",
    "axes[1].set_xticks(range(len(categories)))\n",
    "axes[1].set_xticklabels(categories)\n",
    "axes[1].set_ylabel('Imbalance Ratio')\n",
    "axes[1].set_title('Imbalance Severity Scale', fontweight='bold', fontsize=12)\n",
    "axes[1].legend(loc='upper left')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
